{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Optional Lecture Demo 2: MNIST with TensorFlow\n",
    "\n",
    "\n",
    "Welcome to the (simple) demo for 15112 Optional Lecture on Machine Learning. In this demo we will introduce the concept of Artificial Neural Network, as well as the task of classifying hand-written digits MNIST, widely considered as the \"Hello World\" of Deep Learning. We will implement the demo using [TensorFlow](link to Tensorflow), a popular Machine Learning framework developed by Google. \n",
    "\n",
    "Disclaimer: This demo is not the most comprehensive introduction to ML. It is used in conjunction with the ML Optional Lecture, so there might be details that wouldn't make much sense if you didn't attend the lecture (for example, we went over the neurons, the weights, nonlinear activations, softmax, and backpropagation in lecture). However, there are *many* MNIST tutorials online that might be better suited for your need! \n",
    "\n",
    "\n",
    "## Artificial Neural Networks\n",
    "As discussed in lecture, artificial neural networks are collection of sigmoid units, connected together to mirror the structure of neurons in the brain. \n",
    "\n",
    "Here is a structure of a \"neuron\": \n",
    "\n",
    "\n",
    "![alt text](https://www.eecs.wsu.edu/~cook/dm/lectures/l5/img43.gif)\n",
    "\n",
    "\n",
    "And here is a simple neural network: \n",
    "\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*RGV6Bb3ChmVWsA8Q6Qth6Q.png)\n",
    "\n",
    "\n",
    "We will be using artificial neural network as the main Machine Learning model in this demo. \n",
    "\n",
    "\n",
    "## The Task \n",
    "The task we will be looking at is classifying handwritten digits with [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database. Given an image of a handwritten digits, you want to classify which digit it is among the possible values from 0-9. It is easy to see that with conventional programming paradigm in 112, this is a very tough problem to solve. However, using Neural Networks, we can solve this problem with with very high accuracy. \n",
    "\n",
    "\n",
    "So, let's get started! Run the cell below to import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # for machine learning \n",
    "from matplotlib import pyplot as plt # for graphing and plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has nicely packaged the MNIST dataset for us. The dataset consists of 60,000 training images, and 10,000 testing images. Each image is a 28x28 matrix, with an entry ranging from 0 to 255 (which is usually how image data is represented). Let's load the data and display some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# display shape and image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often time we need to do some kinds of data preprocessing, as the dataset is usually not very \"clean\". For MNIST, we do not need to do much besides normalizing the data (which is usually a good practice). Since the data values range from 0 - 255, we divide each entry by 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "# we can do this instead of a loop because of some numpy magic called \"broadcasting\". Don't worry about it for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model  \n",
    "Let's define the neural network we want to use in this problem. We opted to use a simple neural network, with 1-hidden layer and a dropout layer. The specifications are as follows: \n",
    "\n",
    "1. Input layer is a Image of size 28x28  \n",
    "2. First layer is \"Flatten\", so we flatten a 28x28 matrix into an array of 784 elements \n",
    "3. Second layer is a Hidden Layer (i.e. Dense layer) with 512 hidden units, followed by a ReLu nonactivation function \n",
    "4. Third layer is Dropout. Dropout layer is often used to help with the vanishing/exploding gradient problem. \n",
    "5. Final layer is another Dense layer, with 10 hidden units corresponding to 10 values of digits. Softmax layer is to turn the output of the network into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer, Loss, and Evaluation Metric\n",
    "After defining our model, we need an loss function to optimize, and an optimizer to find the optimum. Here we use Adam optimizer with crossentropy loss. These are fancy techniques that have been shown to work well. The evaluation is accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just trained and evaluated your first neural networks (I would assume... if it's not your first neural network you really should be here...)! \n",
    "\n",
    "Now let's talk about an important aspect of Machine Learning in general, and Neural Network in particular: Hyperparameter Tuning. \n",
    "\n",
    "## Advanced and Optional: Hyperparameter Tuning \n",
    "\n",
    "You might have notice that there are a lot of \"magic numbers\" in your code. For example, why did we choose 512 hidden units in the first hidden layer? Why is the dropout probability 0.2? These numbers are often known as **hyperparameters**. This is to distinguish them from the parameters (i.e. weights) learned by the model. \n",
    "\n",
    "There are a lot of possible values for hyperparameters. Thus, the process of choosing the best hyperparameters is called **hyperparameter tuning**. This is an important step in Machine Learning if you want to achieve competitive results. \n",
    "\n",
    "There are two main ways to do hyperparameter tuning: random search or grid search. Here, we demonstrate grid search with hyperparameter hidden units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = x_train.shape[0] # get the number of images in training set (60,000 images)\n",
    "num_val = int(num_samples*0.1) # determine the validation split \n",
    "(x_val, x_train) = x_train[:num_val], x_train[num_val:]\n",
    "(y_val, y_train) = y_train[:num_val], y_train[num_val:]\n",
    "\n",
    "hidden_units = [32, 64, 128, 256, 512, 1024] # possible values for hidden units \n",
    "val_accuracies = []\n",
    "best_acc, best_unit, best_model = 0, 32, None\n",
    "\n",
    "for unit in hidden_units: \n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(batch_input_shape=(None, 28, 28)),\n",
    "      tf.keras.layers.Dense(unit, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, batch_size=16, epochs=5)\n",
    "    (_, val_acc) = model.evaluate(x_val, y_val)\n",
    "    val_accuracies.append(val_acc)\n",
    "    if (val_acc > best_acc): # get the best model \n",
    "        best_acc = val_acc\n",
    "        best_unit = unit\n",
    "        best_model = model \n",
    "\n",
    "print(\"Best validation accuracy is %f with %d\" % (best_acc, best_unit))\n",
    "\n",
    "# plot validation accuracy \n",
    "plt.xlabel('Number of hidden units')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.plot(hidden_units, val_accuracies, 'r')\n",
    "plt.show()\n",
    "\n",
    "# now, evaluate on test set \n",
    "best_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
