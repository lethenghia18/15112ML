{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Naive Bayes Classifier Demo\n",
    "\n",
    "There are a couple steps we need to take to successfully make our own spam filter. To do this, we'll use a Naive Bayes Classifier, which is one of the most commonly used classifiers for this use case.\n",
    "\n",
    "The first thing we need to do is find a dataset! I've included one that I found here with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2\n",
      "0  spam  Urgent! call 09061749602 from Landline. Your c...\n",
      "1  spam  +449071512431 URGENT! This is the 2nd attempt ...\n",
      "2  spam  FREE for 1st week! No1 Nokia tone 4 ur mob eve...\n",
      "3  spam  Urgent! call 09066612661 from landline. Your c...\n",
      "4  spam  WINNER!! As a valued network customer you have...\n"
     ]
    }
   ],
   "source": [
    "with io.open(\"english_big.txt\", encoding='cp1252') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "sents = []\n",
    "labels = []\n",
    "for line in data:\n",
    "    splits = line.rsplit(',', 1)\n",
    "    sents.append(splits[0])\n",
    "    labels.append(splits[1])\n",
    "\n",
    "df2 = pd.DataFrame({'v1': labels, 'v2': sents}).dropna()\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Transforming Data\n",
    "\n",
    "Now that we have got our dataset, we need to transform both types of data entries, the sentences and the labels, into numbers that our classifier can interpret.\n",
    "\n",
    "For the labels (spam vs. ham), its pretty straightforward, we'll represent spam with a 0 and not-spam, or ham, with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  v1                                                 v2\n",
      "0  0  Urgent! call 09061749602 from Landline. Your c...\n",
      "1  0  +449071512431 URGENT! This is the 2nd attempt ...\n",
      "2  0  FREE for 1st week! No1 Nokia tone 4 ur mob eve...\n",
      "3  0  Urgent! call 09066612661 from landline. Your c...\n",
      "4  0  WINNER!! As a valued network customer you have...\n"
     ]
    }
   ],
   "source": [
    "def transform(L):\n",
    "    res = []\n",
    "    for i in range(len(L)):\n",
    "        if(L[i] == 'spam'):\n",
    "            res.append(0)\n",
    "        else:\n",
    "            res.append(1)\n",
    "    return pd.Series(res)\n",
    "\n",
    "df2['v1'] = transform(df2['v1'])\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, for the X values, we need to somehow convert a sentence into values our classifier can interpret. To do this, we'll use an approach called **bag of words**.\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Bag of words effectively generates a frequency vector for each sentence, which contains word frequencies for each sentence. As a quick small example, consider the following sentences:\n",
    "\n",
    "1. Jon Jon likes ice cream.\n",
    "2. Jeff likes to scream.\n",
    "\n",
    "Bag of words will convert these two sentences into the following:\n",
    "\n",
    "1. {Jon: 2, likes: 1, ice: 1, cream: 1}\n",
    "2. {Jeff: 1, likes: 1, to: 1, scream: 1}\n",
    "\n",
    "This is then further converted into a vector of all possible words as follows:\n",
    "\n",
    "1. [2, 1, 1, 1, 0, 0, 0]\n",
    "2. [0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "Finally, we need to ensure that these weights are normalized, as longer sentences will have higher weights. Thus, we can use a concept called tf-idf to do this, which normalizes the frequencies inside a sentence (thus, above we would have frequences with 2/8, 1/8, etc). Tf-idf also considers the inverse document frequency, or the inverse of the number of documents a word appears in. For more information tf-idf, take a look here: http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/.\n",
    "\n",
    "## Classification\n",
    "\n",
    "After running the Tfidf Vectorizer on our data set, we can now split our dataset into training and testing. From there, we will use the Naive Bayes classifier provided by the sklearn library.\n",
    "\n",
    "We'll specifically be using the MultinomialNB classifier, as our features are discrete. If we were dealing with continous features (like number of words in all caps, for example), we can use a GaussianNB, which would model each probability as a Gaussian variable.\n",
    "\n",
    "Since our features are discrete, we only need to consider their probabilities directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1324, 3294)\n",
      "1324\n",
      "0.982412060302\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(stop_words='english') #ignore words like but, so, etc.\n",
    "X_total = vec.fit_transform(df2['v2'])\n",
    "y_total = np.array(df2['v1']).astype('int')\n",
    "print(X_total.shape)\n",
    "print(len(y_total))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.3, random_state=420)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A note about Accuracy\n",
    "\n",
    "98%! This is a perfect example about why accuracy can be misleading. We've constructed a classifier that may work well for our given dataset - however, there are only 1000 data points that we used. In practice, this is far too little, as 1000 entries can't be used to accurately model all kinds of spam.\n",
    "\n",
    "In the specific case of this type of spam, however, we've achieved pretty high accuracy with a relatively simple classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath (stable)",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}